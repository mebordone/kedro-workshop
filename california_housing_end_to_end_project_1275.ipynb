{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#problema: https://www.kaggle.com/datasets/camnugent/california-housing-prices\n",
    "import pandas as pd\n",
    "housing = pd.read_csv(\"https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/housing/housing.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Analizar los datos\n",
    "# Muestra las primeras 5 filas del DataFrame 'housing' para obtener una visión general de los datos\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestra un resumen conciso del DataFrame 'housing', incluyendo el número de entradas, columnas, tipo de datos y valores no nulos\n",
    "housing.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuenta la cantidad de apariciones de cada valor único en la columna 'ocean_proximity' del DataFrame 'housing'\n",
    "housing['ocean_proximity'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera estadísticas descriptivas del DataFrame 'housing', incluyendo tanto las variables numéricas como categóricas\n",
    "housing.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Genera histogramas para cada variable numérica en el DataFrame 'housing'\n",
    "# 'bins = 50' define el número de intervalos para los histogramas\n",
    "# 'figsize = (15, 10)' ajusta el tamaño de la figura a 15 por 10 pulgadas\n",
    "housing.hist(bins = 50, figsize = (15, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Crear conjuntos de test y train\n",
    "# Stratified sampling: crear categorías que contengan muestras representativas de alguna variable que parezca importante\n",
    "# Miro variables que me interesen\n",
    "\n",
    "# Calcula la matriz de correlación del DataFrame 'housing', mostrando cómo se relacionan entre sí las variables numéricas\n",
    "# Eliminar la columna 'ocean_proximity' y luego calcular la correlación\n",
    "housing.drop('ocean_proximity', axis=1).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'median_income' parece representativa\n",
    "\n",
    "# Crea una nueva categoría 'income_cat' a partir de la columna 'median_income' del DataFrame 'housing'\n",
    "# 'pd.cut' segmenta los datos en intervalos especificados por 'bins'\n",
    "# 'bins=[0, 1.5, 3, 4.5, 6, 16]' define los intervalos de ingreso\n",
    "# 'labels=[1, 2, 3, 4, 5]' asigna una etiqueta a cada intervalo\n",
    "income_cat = pd.cut(housing[\"median_income\"], bins=[0, 1.5, 3, 4.5, 6, 16], labels=[1, 2, 3, 4, 5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Muestra las primeras 5 filas de la nueva categoría 'income_cat' para verificar la segmentación realizada\n",
    "income_cat.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Importa la clase StratifiedShuffleSplit desde sklearn.model_selection\n",
    "# Crea un objeto 'split_object' de StratifiedShuffleSplit con los siguientes parámetros:\n",
    "# - 'n_splits = 1' indica que se realizará una sola división estratificada\n",
    "# - 'test_size = 0.2' establece el tamaño del conjunto de prueba en 20% del total\n",
    "# - 'random_state = 42' fija la semilla aleatoria para asegurar reproducibilidad\n",
    "split_object = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera un generador 'gen_obj' utilizando el objeto 'split_object' para dividir los datos\n",
    "# 'housing' es el conjunto de datos completo\n",
    "# 'income_cat' es la categoría estratificada basada en 'median_income'\n",
    "gen_obj = split_object.split(housing, income_cat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utiliza el generador 'gen_obj' para obtener los índices de entrenamiento y prueba. \n",
    "# Los índices se refieren a las posiciones o ubicaciones de las filas dentro del DataFrame housing.\n",
    "train_ind, test_ind = next(gen_obj)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea el conjunto de entrenamiento utilizando los índices de entrenamiento obtenidos anteriormente\n",
    "strat_train_set = housing.loc[train_ind]\n",
    "\n",
    "# Crea el conjunto de prueba utilizando los índices de prueba obtenidos anteriormente\n",
    "strat_test_set = housing.loc[test_ind]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar los datos más profundamente\n",
    "\n",
    "# Crea una copia del conjunto de entrenamiento 'strat_train_set' para realizar análisis exploratorio y manipulaciones sin afectar el original\n",
    "train_copy = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Genera un gráfico de dispersión para visualizar la distribución geográfica de los datos\n",
    "\n",
    "train_copy.plot(kind=\"scatter\",          # Tipo de gráfico: dispersión\n",
    "                x=\"latitude\",            # Variable en el eje x: latitud\n",
    "                y=\"longitude\",           # Variable en el eje y: longitud\n",
    "                alpha=0.4,               # Transparencia de los puntos\n",
    "                s=train_copy[\"population\"]/100,  # Tamaño de los puntos basado en la población\n",
    "                c=\"median_house_value\",  # Color de los puntos basado en el valor medio de la vivienda\n",
    "                cmap=plt.get_cmap(\"jet\"),# Mapa de colores utilizado (jet)\n",
    "                colorbar=True,           # Mostrar barra de colores\n",
    "                figsize=(10,7)           # Tamaño de la figura (10 pulgadas de ancho, 7 pulgadas de alto)\n",
    "               )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera una matriz de gráficos de dispersión para explorar las relaciones entre múltiples variables\n",
    "\n",
    "pd.plotting.scatter_matrix(strat_train_set[[\"median_house_value\", \"median_income\", \"latitude\", \"total_rooms\", \"housing_median_age\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genera una matriz de gráficos de dispersión para explorar la relación entre 'median_house_value' y 'median_income'\n",
    "\n",
    "pd.plotting.scatter_matrix(strat_train_set[[\"median_house_value\", \"median_income\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear nuevas variables combinando información en el conjunto de entrenamiento 'train_copy'\n",
    "\n",
    "# Variable que representa el número promedio de habitaciones por hogar\n",
    "train_copy[\"rooms_per_household\"] = train_copy[\"total_rooms\"] / train_copy[\"households\"]\n",
    "\n",
    "# Variable que representa el porcentaje de dormitorios en relación al número total de habitaciones\n",
    "train_copy[\"bedrooms_per_room\"] = train_copy[\"total_bedrooms\"] / train_copy[\"total_rooms\"]\n",
    "\n",
    "# Variable que representa la relación entre la población y el valor medio de la vivienda por hogar\n",
    "train_copy[\"population_per_household\"] = train_copy[\"population\"] / train_copy[\"median_house_value\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_copy.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea un conjunto de datos de entrenamiento eliminando la columna 'median_house_value'\n",
    "\n",
    "train_data = strat_train_set.drop(\"median_house_value\", axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crea una copia de la columna 'median_house_value' del conjunto de entrenamiento como etiquetas\n",
    "\n",
    "housing_labels = strat_train_set['median_house_value'].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transformaciones de los atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manejar valores nulos utilizando SimpleImputer de sklearn\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Crea un objeto SimpleImputer con estrategia de imputación mediana\n",
    "imputer = SimpleImputer(strategy=\"median\")\n",
    "\n",
    "# Elimina la columna 'ocean_proximity' del conjunto de datos de entrenamiento 'train_data' para trabajar solo con datos numéricos\n",
    "housing_num = train_data.drop(\"ocean_proximity\", axis=1)\n",
    "\n",
    "# Ajusta el imputer utilizando los datos de entrenamiento 'housing_num' (calcula la mediana en este caso por cada columna)\n",
    "imputer.fit(housing_num)\n",
    "\n",
    "# Transforma los datos numéricos 'housing_num' imputando los valores nulos con la mediana\n",
    "out = imputer.transform(housing_num)\n",
    "\n",
    "# Crea un nuevo DataFrame 'housing_tr' con los datos transformados y las columnas originales de 'housing_num'\n",
    "housing_tr = pd.DataFrame(out, columns=housing_num.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Genera estadísticas descriptivas del DataFrame 'housing_tr' para analizar los datos transformados\n",
    "\n",
    "housing_tr.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convertir variables no numericas a numericas- >  One Hot Encoding categories \n",
    "\"\"\"\n",
    "    '<1H OCEAN' - [1,0,0,0,0]\n",
    "    ‘INLAND’ - [0,1,0,0,0]\n",
    "    ‘ISLAND’ - [0,0,1,0,0]\n",
    "    ‘NEAR BAY’ - [0,0,0,1,0]\n",
    "    ‘NEAR OCEAN' - [0,0,0,0,1]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar la clase OneHotEncoder desde sklearn.preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Crear un objeto OneHotEncoder\n",
    "encoder = OneHotEncoder()\n",
    "\n",
    "# Aplicar One Hot Encoding a la columna 'ocean_proximity' del conjunto de datos de entrenamiento 'train_data'\n",
    "one_hot = encoder.fit_transform(train_data[['ocean_proximity']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# El resultado de one_hot es una matriz dispersa (sparse matrix) que contiene la representación \n",
    "# one-hot encoded de la columna 'ocean_proximity'. \n",
    "print(one_hot.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combinar atributos - armando pipelines y transformadores manuales\n",
    "\n",
    "# Importar las clases necesarias\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "# Definir la clase del transformador personalizado\n",
    "class CombinedAttributesAdder(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Añade atributos combinados al conjunto de datos.\n",
    "    \n",
    "    Parámetros:\n",
    "    ----------\n",
    "    add_bedrooms_per_room : bool, opcional (default=True)\n",
    "        Indica si se debe añadir el atributo 'bedrooms_per_room'.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, add_bedrooms_per_room=True):\n",
    "        self.add_bedrooms_per_room = add_bedrooms_per_room\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Método de ajuste del transformador.\n",
    "        \n",
    "        Parámetros:\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Conjunto de datos de entrenamiento.\n",
    "        y : array-like, shape (n_samples,), opcional (default=None)\n",
    "            Etiquetas de entrenamiento.\n",
    "        \n",
    "        Retorna:\n",
    "        -------\n",
    "        self : CombinedAttributesAdder\n",
    "            Devuelve la instancia del transformador.\n",
    "        \"\"\"\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        \"\"\"\n",
    "        Método de transformación del conjunto de datos.\n",
    "        \n",
    "        Parámetros:\n",
    "        ----------\n",
    "        X : array-like, shape (n_samples, n_features)\n",
    "            Conjunto de datos a transformar.\n",
    "        y : array-like, shape (n_samples,), opcional (default=None)\n",
    "            Etiquetas de los datos, no se utilizan en este transformador.\n",
    "        \n",
    "        Retorna:\n",
    "        -------\n",
    "        X_transformed : array-like, shape (n_samples, n_features + 2)\n",
    "            Conjunto de datos transformado con los atributos adicionales.\n",
    "        \"\"\"\n",
    "        rooms_per_household = X[:, 3] / X[:, 6]  # Calcula 'rooms_per_household'\n",
    "        population_per_household = X[:, 5] / X[:, 6]  # Calcula 'population_per_household'\n",
    "        \n",
    "        if self.add_bedrooms_per_room:\n",
    "            bedrooms_per_room = X[:, 4] / X[:, 3]  # Calcula 'bedrooms_per_room'\n",
    "            X_transformed = np.c_[X, rooms_per_household, population_per_household, bedrooms_per_room]\n",
    "        else:\n",
    "            X_transformed = np.c_[X, rooms_per_household, population_per_household]\n",
    "        \n",
    "        return X_transformed\n",
    "\n",
    "# Ejemplo de uso del transformador personalizado\n",
    "attr_adder = CombinedAttributesAdder(add_bedrooms_per_room=False)\n",
    "housing_extra_attribs = attr_adder.transform(train_data.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hay que escalar los valores de las columnas\n",
    "# Minmax scaling mapa features al rango de de 0 a 1\n",
    "# nv = (value - min / max-min)\n",
    "# Standarizacion: Mide en desviaciones estandard del valor original\n",
    "# le quita peso a los outliers\n",
    "\n",
    "# Importar la clase StandardScaler desde sklearn.preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Crear un objeto StandardScaler para escalar las características\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Aplicar el escalado a las características numéricas 'housing_num'\n",
    "scaled_features = scaler.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encadenar transformadores (hacer un pipeline)\n",
    "from sklearn.pipeline import Pipeline\n",
    "num_pipeline = Pipeline([\n",
    "          ('imputer',SimpleImputer(strategy=\"median\")),\n",
    "          ('attribs_adder',CombinedAttributesAdder()),\n",
    "            (\"std_scaler\", StandardScaler())\n",
    "                ])\n",
    "housing_num_tr = num_pipeline.fit_transform(housing_num)\n",
    "\n",
    "#Transformadores se aplican a todas las columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Como Componer las salidas de todas las transformaciones\n",
    "\n",
    "# Importar la clase ColumnTransformer desde sklearn.compose y OneHotEncoder desde sklearn.preprocessing\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Definir el pipeline completo utilizando ColumnTransformer\n",
    "full_pipeline = ColumnTransformer([\n",
    "    (\"num\", num_pipeline, list(housing_num)),  # Pipeline numérico definido anteriormente\n",
    "    (\"cat\", OneHotEncoder(), [\"ocean_proximity\"])  # Transformador OneHotEncoder para la columna categórica\n",
    "])\n",
    "\n",
    "# Aplicar el pipeline completo a los datos de entrenamiento 'train_data'\n",
    "housing_prepared = full_pipeline.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar la clase LinearRegression desde sklearn.linear_model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Crear una instancia del modelo de regresión lineal\n",
    "lin_reg = LinearRegression()\n",
    "\n",
    "# Entrenar el modelo utilizando los datos preparados 'housing_prepared' y las etiquetas 'housing_labels'\n",
    "lin_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "# Realizar predicciones utilizando el mismo conjunto de datos de entrenamiento\n",
    "predictions = lin_reg.predict(housing_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimir las primeras 5 predicciones y las primeras 5 etiquetas reales\n",
    "print(\"Predicciones:\", predictions[:5])\n",
    "print(\"Etiquetas reales:\", list(housing_labels[:5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Medir el error\n",
    "\n",
    "# Importar la función mean_squared_error desde sklearn.metrics -> error cuadrático medio entre las etiquetas reales y las predicciones.\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Calcular el (RMSE entre las etiquetas reales 'housing_labels' y las predicciones 'predictions'\n",
    "lin_rmse = mean_squared_error(housing_labels, predictions, squared=False)\n",
    "\n",
    "# Imprimir el resultado del RMSE\n",
    "print(\"RMSE del modelo de regresión lineal:\", lin_rmse)\n",
    "\n",
    "#el error es alto probablemente el problema no se resuelva con una aproximacion lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probemos con arboles de desicion\n",
    "\n",
    "# Importar la clase DecisionTreeRegressor desde sklearn.tree\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Crear una instancia del modelo de árbol de decisión\n",
    "tree_reg = DecisionTreeRegressor()\n",
    "\n",
    "# Entrenar el modelo utilizando los datos preparados 'housing_prepared' y las etiquetas 'housing_labels'\n",
    "tree_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "# Realizar predicciones utilizando el mismo conjunto de datos de entrenamiento\n",
    "predictions = tree_reg.predict(housing_prepared)\n",
    "\n",
    "# Medir el error utilizando el RMSE\n",
    "from sklearn.metrics import mean_squared_error\n",
    "tree_rmse = mean_squared_error(housing_labels, predictions, squared=False)\n",
    "\n",
    "# Imprimir el resultado del RMSE\n",
    "print(\"RMSE del modelo de árbol de decisión:\", tree_rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"¿Hemos logrado un modelo perfecto?\n",
    "Obviamente no, porque no existe tal cosa como un modelo perfecto.\n",
    "Esto significa que nuestro modelo probablemente está sobreajustando.\n",
    "\"\"\"\n",
    "\n",
    "# Hagamos un testeo mas interesante: Cross validation\n",
    "\n",
    "# Importar la función cross_val_score desde sklearn.model_selection\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Aplicar validación cruzada con 10 folds y usar \"neg_root_mean_squared_error\" como métrica de puntuación\n",
    "scores = cross_val_score(tree_reg, housing_prepared, housing_labels, scoring=\"neg_root_mean_squared_error\", cv=10)\n",
    "\n",
    "# Tomar el valor absoluto de los scores (ya que cross_val_score devuelve negativos para maximizar)\n",
    "scores = abs(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores)\n",
    "# veo que funciona casi peor que la regresion lineal!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# veo que funciona casi peor que la regresion lineal!\n",
    "\n",
    "# Probemos con random forest (bosques aleatorios)\n",
    "\n",
    "# Importar la clase RandomForestRegressor desde sklearn.ensemble\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Crear una instancia del modelo de Random Forest\n",
    "forest_reg = RandomForestRegressor()\n",
    "\n",
    "# Entrenar el modelo de Random Forest utilizando los datos preparados 'housing_prepared' y las etiquetas 'housing_labels'\n",
    "forest_reg.fit(housing_prepared, housing_labels)\n",
    "\n",
    "# Realizar predicciones utilizando el mismo conjunto de datos de entrenamiento\n",
    "predictions = forest_reg.predict(housing_prepared)\n",
    "\n",
    "# Medir el error utilizando el RMSE con los datos de entrenamiento\n",
    "from sklearn.metrics import mean_squared_error\n",
    "forest_rmse = mean_squared_error(housing_labels, predictions, squared=False)\n",
    "print(\"RMSE del modelo de Random Forest (datos de entrenamiento):\", forest_rmse)\n",
    "\n",
    "# Medir el error utilizando la validación cruzada con 10 folds\n",
    "scores = cross_val_score(forest_reg, housing_prepared, housing_labels, scoring=\"neg_root_mean_squared_error\", cv=10)\n",
    "scores = abs(scores)\n",
    "print(\"Scores de RMSE (validación cruzada):\", scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = abs(scores)\n",
    "# Tenemos un modelo mas piola, antes de optimizar los parametros del modelo podemos probar si no hay otro que sea prometedor, pero en este caso nos vamos a quedar con este.\n",
    "# Support Vector Machines, possibly a neural network, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine Tuning\n",
    "# probar con diferentes parametros nuestro modelo para ver si mejora\n",
    "# GridSearchCV busca por todas als combinaciones podibles de parametros\n",
    "# GridSearchCV(estimator, param_grid:dict, cv = None, scoring = None)\n",
    "\n",
    "# Importar la clase GridSearchCV desde sklearn.model_selection\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Crear una instancia del modelo de Random Forest\n",
    "reg_forest = RandomForestRegressor()\n",
    "\n",
    "# Definir el diccionario de parámetros que se probarán\n",
    "param_grid = {\n",
    "    'n_estimators': [3, 10, 30],     # Número de árboles en el bosque\n",
    "    'max_features': [2, 4, 6, 8]      # Máximo número de características a considerar en cada split\n",
    "}\n",
    "\n",
    "# Configurar GridSearchCV con el modelo, los parámetros, 5 folds de validación cruzada y la métrica de RMSE negativa\n",
    "grid_search = GridSearchCV(reg_forest, param_grid, cv=5, scoring=\"neg_root_mean_squared_error\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejecutar la búsqueda de hiperparámetros utilizando GridSearchCV\n",
    "grid_search.fit(housing_prepared, housing_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = grid_search.best_params_\n",
    "print(\"Mejores parámetros encontrados:\", best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para ver los resultados de todas als combinaciones\n",
    "# Obtener los resultados de la búsqueda de hiperparámetros\n",
    "cvres = grid_search.cv_results_\n",
    "\n",
    "# Iterar sobre los resultados para imprimir el score medio y los parámetros de cada combinación\n",
    "for mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n",
    "    print(\"RMSE:\", -mean_score, \" | Parámetros:\", params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar el modelo en el conjunto de prueba\n",
    "\n",
    "# Obtener X_test y y_test del conjunto de prueba\n",
    "X_test = strat_test_set.drop(\"median_house_value\", axis=1)\n",
    "y_test = strat_test_set[\"median_house_value\"].copy()\n",
    "\n",
    "# Aplicar la transformación completa a X_test\n",
    "X_test_prepared = full_pipeline.transform(X_test)\n",
    "\n",
    "# Obtener el mejor modelo encontrado por GridSearchCV\n",
    "final_model = grid_search.best_estimator_\n",
    "\n",
    "# Realizar predicciones sobre X_test_prepared utilizando el modelo final\n",
    "final_predictions = final_model.predict(X_test_prepared)\n",
    "\n",
    "# Calcular el RMSE entre las etiquetas reales (y_test) y las predicciones finales\n",
    "final_rmse = mean_squared_error(y_test, final_predictions, squared=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"RMSE final en el conjunto de prueba:\", final_rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
